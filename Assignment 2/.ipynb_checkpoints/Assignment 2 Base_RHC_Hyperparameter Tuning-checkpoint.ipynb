{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f47090e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\marcu\\desktop\\ga tech\\ml\\ml-sp24\\mlrose\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\marcu\\anaconda3\\lib\\site-packages (from mlrose-hiive==2.2.4) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\marcu\\anaconda3\\lib\\site-packages (from mlrose-hiive==2.2.4) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\marcu\\anaconda3\\lib\\site-packages (from mlrose-hiive==2.2.4) (1.3.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\marcu\\anaconda3\\lib\\site-packages (from mlrose-hiive==2.2.4) (1.5.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\marcu\\anaconda3\\lib\\site-packages (from mlrose-hiive==2.2.4) (3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\marcu\\anaconda3\\lib\\site-packages (from mlrose-hiive==2.2.4) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\marcu\\anaconda3\\lib\\site-packages (from pandas->mlrose-hiive==2.2.4) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\marcu\\anaconda3\\lib\\site-packages (from pandas->mlrose-hiive==2.2.4) (2022.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\marcu\\anaconda3\\lib\\site-packages (from scikit-learn->mlrose-hiive==2.2.4) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\marcu\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->mlrose-hiive==2.2.4) (1.16.0)\n",
      "Building wheels for collected packages: mlrose-hiive\n",
      "  Building wheel for mlrose-hiive (setup.py): started\n",
      "  Building wheel for mlrose-hiive (setup.py): finished with status 'done'\n",
      "  Created wheel for mlrose-hiive: filename=mlrose_hiive-2.2.4-py3-none-any.whl size=103936 sha256=35a72eb28f9096d8fff6bb75f8157c95e13a40484356302da2840e5d274cc010\n",
      "  Stored in directory: C:\\Users\\marcu\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-un4wrtcf\\wheels\\1b\\71\\20\\a26b3f48984937d91461af271cddc3ba8371491b048e63bbfa\n",
      "Successfully built mlrose-hiive\n",
      "Installing collected packages: mlrose-hiive\n",
      "  Attempting uninstall: mlrose-hiive\n",
      "    Found existing installation: mlrose-hiive 2.2.4\n",
      "    Uninstalling mlrose-hiive-2.2.4:\n",
      "      Successfully uninstalled mlrose-hiive-2.2.4\n",
      "Successfully installed mlrose-hiive-2.2.4\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import random\n",
    "import pandas as pd\n",
    "import sys\n",
    "from platform import python_version\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV,train_test_split,cross_val_score, KFold, StratifiedKFold, learning_curve\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, recall_score, precision_recall_fscore_support, make_scorer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import time\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# ! pip install xgboost\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras import backend as K\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from keras.optimizers import Adam, SGD\n",
    "! pip install \"C:\\Users\\marcu\\Desktop\\GA Tech\\ML\\ML-SP24\\mlrose\"\n",
    "import mlrose_hiive\n",
    "from mlrose_hiive.runners import SKMLPRunner\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5c27e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_ingestion():\n",
    "    # Ingesting Wine Quality Dataset\n",
    "    wine = pd.read_csv(r'C:\\Users\\marcu\\Desktop\\GA Tech\\ML\\ML-SP24\\Assignment 2\\winequality-white.csv')\n",
    "    column_names = wine.columns.tolist()\n",
    "    # Standardize the values in all columns\n",
    "    scaler = StandardScaler()\n",
    "    columns_stand = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
    "    wine[columns_stand] = scaler.fit_transform(wine[columns_stand])\n",
    "    \n",
    "    # Encoding quality wine \n",
    "    label_encoder = LabelEncoder()\n",
    "    wine['encoded_quality'] = label_encoder.fit_transform(wine['quality'])\n",
    "    wine = wine.drop(columns=['quality'])\n",
    "    wine = wine.rename(columns={'encoded_quality': 'quality'})\n",
    "\n",
    "\n",
    "    # Descriptive Stats for Wine\n",
    "    df_wine = wine.describe()\n",
    "    df_wine.to_csv('descriptive/descriptive_stats_wine.csv')\n",
    "\n",
    "    # Output boxplot for outlier distribution\n",
    "    column_names = wine.columns.tolist()\n",
    "    for i in column_names:\n",
    "        column_data = wine[i].values\n",
    "        # Create boxplot using Matplotlib\n",
    "        plt.clf()\n",
    "        plt.boxplot(column_data)\n",
    "        plt.title(f'Box and Whisker Plot for Wine Quality for {i}')\n",
    "        plt.savefig(f'image/wine_{i}.png')\n",
    "\n",
    "    # Plotting Target Variable for Apple Quality\n",
    "    plt.clf()\n",
    "#     plt.hist(wine['quality'], label=i)\n",
    "    plt.savefig('image/wine_target.png', bbox_inches='tight')\n",
    "    \n",
    "    # Ingesting Apple Quality\n",
    "    aq = pd.read_csv(r'C:\\Users\\marcu\\Desktop\\GA Tech\\ML\\ML-SP24\\Assignment 2\\apple_quality.csv')\n",
    "    # Descriptive Stats for Wine\n",
    "    df_aq = aq.describe()\n",
    "    df_aq.to_csv('descriptive/descriptive_stats_aq.csv')\n",
    "    \n",
    "    # Output boxplot for outlier distribution\n",
    "    column_names = aq.columns.tolist()\n",
    "    for i in column_names:\n",
    "        column_data = aq[i].values\n",
    "        # Create boxplot using Matplotlib\n",
    "        plt.clf()\n",
    "        plt.boxplot(column_data)\n",
    "        plt.title(f'Box and Whisker Plot for Apple Quality for {i}')\n",
    "        plt.savefig(f'image/aq_{i}.png')\n",
    "\n",
    "    # Plotting Target Variable for Apple Quality\n",
    "    plt.clf()\n",
    "#     plt.hist(aq['Quality'], label=i)\n",
    "    plt.savefig('image/apple_target.png', bbox_inches='tight')\n",
    "    \n",
    "    return(wine,aq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba1dd345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(wine, aq):\n",
    "    list_w = wine.columns.tolist()\n",
    "    list_w.remove('quality')\n",
    "    x_train_w, x_test_w, y_train_w, y_test_w = train_test_split(wine[list_w], wine['quality'], test_size=0.3, random_state=42)\n",
    "\n",
    "    list_a = aq.columns.tolist()\n",
    "    list_a.remove('Quality')\n",
    "    x_train_a, x_test_a, y_train_a, y_test_a = train_test_split(aq[list_a], aq['Quality'], test_size=0.3, random_state=42)\n",
    "\n",
    "    return(x_train_w, x_test_w, y_train_w, y_test_w, x_train_a, x_test_a, y_train_a, y_test_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae49d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strat_kfold(clf, num_folds, x_train, y_train):\n",
    "    # Create a StratifiedKFold object\n",
    "    stratified_kfold = StratifiedKFold(n_splits=num_folds, shuffle=True,random_state = 42)\n",
    "    \n",
    "    # Lists to store training and validation scores for each fold\n",
    "    training_scores = []\n",
    "    validation_scores = []\n",
    "    \n",
    "    # Iterate over folds\n",
    "    for fold_num, (train_index, val_index) in enumerate(stratified_kfold.split(x_train, y_train), start=1):\n",
    "\n",
    "\n",
    "        # Extract training and validation sets for this fold\n",
    "        X_train_sub, X_val_sub = x_train.iloc[train_index], x_train.iloc[val_index]\n",
    "        y_train_sub, y_val_sub = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "#         X_train_sub.to_csv(f'{fold_num}_x_train_sub.csv')\n",
    "#         X_val_sub.to_csv(f'{fold_num}_x_val_sub.csv')\n",
    "#         y_train_sub.to_csv(f'{fold_num}_y_train_sub.csv')\n",
    "#         y_val_sub.to_csv(f'{fold_num}_y_val_sub.csv')\n",
    "        \n",
    "        # Train the Decision Tree model\n",
    "        clf.fit(X_train_sub, y_train_sub)\n",
    "\n",
    "        # Make predictions on the training set\n",
    "        y_train_pred = clf.predict(X_train_sub)\n",
    "\n",
    "        # Make predictions on the validation set\n",
    "        y_val_pred = clf.predict(X_val_sub)\n",
    "\n",
    "        # Calculate recall scores for training set\n",
    "        _, recall_train, _, _ = precision_recall_fscore_support(y_train_sub, y_train_pred, average='micro')\n",
    "        training_scores.append(recall_train)\n",
    "\n",
    "        # Calculate recall scores for validation sets\n",
    "        _, recall_val, _, _ = precision_recall_fscore_support(y_val_sub, y_val_pred, average='micro')\n",
    "        validation_scores.append(recall_val)\n",
    "        \n",
    "        \n",
    "#         weighted_recall = sum(recall * (sum(y_true == i) / len(y_true)) for i in set(y_true))\n",
    "\n",
    "\n",
    "        # Print scores for the current fold\n",
    "#         print(f'Fold {fold_num}: Training Recall = {recall_train:.4f}, Validation Recall = {recall_val:.4f}')\n",
    "\n",
    "    # Print mean scores across all folds\n",
    "    mean_training_score = np.mean(training_scores)\n",
    "    mean_validation_score = np.mean(validation_scores)\n",
    "#     print(mean_validation_score, mean_training_score)\n",
    "    return(mean_validation_score, mean_training_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6c1a7895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strat_kfold_ann(clf, num_folds, x_train, y_train, dataset = 'wine', epochs = 5):\n",
    "    # Create a StratifiedKFold object\n",
    "    stratified_kfold = StratifiedKFold(n_splits=num_folds, shuffle=True,random_state = 42)\n",
    "    \n",
    "    # Lists to store training and validation scores for each fold\n",
    "    training_scores = []\n",
    "    validation_scores = []\n",
    "    \n",
    "    # Iterate over folds\n",
    "    for fold_num, (train_index, val_index) in enumerate(stratified_kfold.split(x_train, y_train), start=1):\n",
    "\n",
    "\n",
    "        # Extract training and validation sets for this fold\n",
    "        X_train_sub, X_val_sub = x_train.iloc[train_index], x_train.iloc[val_index]\n",
    "        y_train_sub, y_val_sub = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        \n",
    "        # One hot encoding\n",
    "        encoder = OneHotEncoder(sparse=False)\n",
    "        y_train_hot = encoder.fit_transform(y_train_sub.values.reshape(-1, 1))\n",
    "        y_val_hot = encoder.transform(y_val_sub.values.reshape(-1, 1))\n",
    "        \n",
    "        \n",
    "        # Train the Decision Tree model\n",
    "        clf.fit(X_train_sub, y_train_hot)\n",
    "\n",
    "        if dataset == 'wine':\n",
    "            # Make predictions on the training set\n",
    "            predictions_train = clf.predict(X_train_sub)\n",
    "            y_train_pred = np.argmax(predictions_train, axis=1)\n",
    "            \n",
    "            # Converting y_train_sub into the quality category \n",
    "            y_train_sub = np.argmax(y_train_hot, axis=1)\n",
    "            \n",
    "            # Make predictions on the validation set\n",
    "            predictions_val = clf.predict(X_val_sub)\n",
    "            y_val_pred = np.argmax(predictions_val, axis=1)\n",
    "            \n",
    "            # Converting y_train_sub into the quality category \n",
    "            y_val_sub = np.argmax(y_val_hot, axis=1)\n",
    "\n",
    "        else:            \n",
    "            # Make predictions on the training set\n",
    "            predictions_train = clf.predict(X_train_sub)\n",
    "            y_train_pred = np.argmax(predictions_train, axis=1)\n",
    "            \n",
    "            # Converting y_train_sub into the quality category \n",
    "            y_train_sub = np.argmax(y_train_hot, axis=1)\n",
    "            \n",
    "            # Make predictions on the validation set\n",
    "            predictions_val = clf.predict(X_val_sub)\n",
    "            y_val_pred = np.argmax(predictions_val, axis=1)\n",
    "            \n",
    "            # Converting y_train_sub into the quality category \n",
    "            y_val_sub = np.argmax(y_val_hot, axis=1)\n",
    "\n",
    "        # Calculate recall scores for training set\n",
    "        _, recall_train, _, _ = precision_recall_fscore_support(y_train_sub, y_train_pred, average='micro')\n",
    "        training_scores.append(recall_train)\n",
    "\n",
    "        # Calculate recall scores for validation sets\n",
    "        _, recall_val, _, _ = precision_recall_fscore_support(y_val_sub, y_val_pred, average='micro')\n",
    "        validation_scores.append(recall_val)\n",
    "\n",
    "    # Print mean scores across all folds\n",
    "    mean_training_score = np.mean(training_scores)\n",
    "    mean_validation_score = np.mean(validation_scores)\n",
    "#     print(mean_validation_score, mean_training_score)\n",
    "    return(mean_validation_score, mean_training_score)\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "09bfa2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data ingestion\n",
    "wine,aq = data_ingestion()\n",
    "x_train_w, x_test_w, y_train_w, y_test_w, x_train_a, x_test_a, y_train_a, y_test_a = train_test(wine,aq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30519cc4",
   "metadata": {},
   "source": [
    "# Optimal Parameters for Base Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fb6880ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes:  [64, 64, 64] activations:  sigmoid LR:  0.01 validation recall:  0.27706624672809693 training recall:  0.27707238609890356\n",
      "40.80046629905701\n",
      "nodes:  [64, 64, 64] activations:  sigmoid LR:  0.001 validation recall:  0.4991325998595476 training recall:  0.5218054683588196\n",
      "657.7781021595001\n",
      "nodes:  [64, 64, 64] activations:  sigmoid LR:  0.0001 validation recall:  0.533843076333766 training recall:  0.5780332032761332\n",
      "2863.7593874931335\n",
      "nodes:  [64, 64, 64] activations:  relu LR:  0.01 validation recall:  0.0037917899172181906 training recall:  0.0038652063883586178\n",
      "11.651906967163086\n",
      "nodes:  [64, 64, 64] activations:  relu LR:  0.001 validation recall:  0.0037917899172181906 training recall:  0.0038652063883586178\n",
      "125.86319208145142\n",
      "nodes:  [64, 64, 64] activations:  relu LR:  0.0001 validation recall:  0.1700423485348258 training recall:  0.23870769252042132\n",
      "594.4133622646332\n",
      "nodes:  [64, 64, 64] activations:  tanh LR:  0.01 validation recall:  0.10765167798088995 training recall:  0.1080794744955198\n",
      "1028.0362200737\n",
      "nodes:  [64, 64, 64] activations:  tanh LR:  0.001 validation recall:  0.320925283564938 training recall:  0.3186925515329385\n",
      "417.85809540748596\n",
      "nodes:  [64, 64, 64] activations:  tanh LR:  0.0001 validation recall:  0.577596561043604 training recall:  0.9997082687501345\n",
      "408.18867349624634\n",
      "nodes:  [64, 64, 64, 64] activations:  sigmoid LR:  0.01 validation recall:  0.2693149752080185 training recall:  0.2702590480961684\n",
      "12242.368982076645\n",
      "nodes:  [64, 64, 64, 64] activations:  sigmoid LR:  0.001 validation recall:  0.4883411717137324 training recall:  0.48672414072768744\n",
      "904.6215765476227\n",
      "nodes:  [64, 64, 64, 64] activations:  sigmoid LR:  0.0001 validation recall:  0.5274316358451618 training recall:  0.5751173266983154\n",
      "898.5545046329498\n",
      "nodes:  [64, 64, 64, 64] activations:  relu LR:  0.01 validation recall:  0.0037917899172181906 training recall:  0.004011032126601417\n",
      "14.859330177307129\n",
      "nodes:  [64, 64, 64, 64] activations:  relu LR:  0.001 validation recall:  0.0037917899172181906 training recall:  0.0038651797972320233\n",
      "14.528669595718384\n",
      "nodes:  [64, 64, 64, 64] activations:  relu LR:  0.0001 validation recall:  0.013415334851354512 training recall:  0.013492284451663047\n",
      "689.2387568950653\n",
      "nodes:  [64, 64, 64, 64] activations:  tanh LR:  0.01 validation recall:  0.1266033921389202 training recall:  0.12660436897528168\n",
      "531.4354393482208\n",
      "nodes:  [64, 64, 64, 64] activations:  tanh LR:  0.001 validation recall:  0.1266033921389202 training recall:  0.12660436897528168\n",
      "600.0032680034637\n",
      "nodes:  [64, 64, 64, 64] activations:  tanh LR:  0.0001 validation recall:  0.5953940116192462 training recall:  0.9850501495352004\n",
      "732.4921119213104\n",
      "nodes:  [128, 128, 128] activations:  sigmoid LR:  0.01 validation recall:  0.2744827732970143 training recall:  0.27443648749299654\n",
      "45.5411913394928\n",
      "nodes:  [128, 128, 128] activations:  sigmoid LR:  0.001 validation recall:  0.4740558830414335 training recall:  0.4749811535390263\n",
      "1474.629610300064\n",
      "nodes:  [128, 128, 128] activations:  sigmoid LR:  0.0001 validation recall:  0.5277193505139282 training recall:  0.5964128304313108\n",
      "1596.24978017807\n",
      "nodes:  [128, 128, 128] activations:  relu LR:  0.01 validation recall:  0.0037917899172181906 training recall:  0.003938092666353424\n",
      "22.228343963623047\n",
      "nodes:  [128, 128, 128] activations:  relu LR:  0.001 validation recall:  0.0037917899172181906 training recall:  0.0037922669281106234\n",
      "23.07377552986145\n",
      "nodes:  [128, 128, 128] activations:  relu LR:  0.0001 validation recall:  0.006124151433253176 training recall:  0.008314513463486262\n",
      "228.95297169685364\n",
      "nodes:  [128, 128, 128] activations:  tanh LR:  0.01 validation recall:  0.1907854695580005 training recall:  0.1907820264193479\n",
      "167.23482012748718\n",
      "nodes:  [128, 128, 128] activations:  tanh LR:  0.001 validation recall:  0.1907854695580005 training recall:  0.1907820264193479\n",
      "654.8020632266998\n",
      "nodes:  [128, 128, 128] activations:  tanh LR:  0.0001 validation recall:  0.5840199187078376 training recall:  1.0\n",
      "914.3063061237335\n",
      "nodes:  [128, 128, 128, 128] activations:  sigmoid LR:  0.01 validation recall:  0.12282905237173074 training recall:  0.12251606303479741\n",
      "1449.880514383316\n",
      "nodes:  [128, 128, 128, 128] activations:  sigmoid LR:  0.001 validation recall:  0.45858015364644295 training recall:  0.45901382020622483\n",
      "1240.5971088409424\n",
      "nodes:  [128, 128, 128, 128] activations:  sigmoid LR:  0.0001 validation recall:  0.5297614436807049 training recall:  0.5915258600035684\n",
      "1740.7706182003021\n",
      "nodes:  [128, 128, 128, 128] activations:  relu LR:  0.01 validation recall:  0.0037917899172181906 training recall:  0.0037922669281106234\n",
      "39.00009608268738\n",
      "nodes:  [128, 128, 128, 128] activations:  relu LR:  0.001 validation recall:  0.0037917899172181906 training recall:  0.004011005535474824\n",
      "364.4841833114624\n",
      "nodes:  [128, 128, 128, 128] activations:  relu LR:  0.0001 validation recall:  0.005834308697410142 training recall:  0.006563381412749328\n",
      "916.0241639614105\n",
      "nodes:  [128, 128, 128, 128] activations:  tanh LR:  0.01 validation recall:  0.10003702836713413 training recall:  0.09991772705431742\n",
      "976.2162046432495\n",
      "nodes:  [128, 128, 128, 128] activations:  tanh LR:  0.001 validation recall:  0.12397863420655017 training recall:  0.12397897386437942\n",
      "1173.4915704727173\n",
      "nodes:  [128, 128, 128, 128] activations:  tanh LR:  0.0001 validation recall:  0.5802200421357281 training recall:  0.9631004509057337\n",
      "1031.8318343162537\n"
     ]
    }
   ],
   "source": [
    "nodes = [[64,64,64],[64,64,64,64],[128,128,128],[128,128,128,128]]\n",
    "activation = ['sigmoid','relu','tanh']\n",
    "learning_rate = [0.01,0.001,0.0001]\n",
    "\n",
    "for i in range(0,len(nodes)):\n",
    "    for x in range(0,len(activation)):\n",
    "        for y in range(0,len(learning_rate)):\n",
    "            # Base Case\n",
    "            start_time = time.time()\n",
    "            nn_model_base = mlrose_hiive.NeuralNetwork(hidden_nodes = nodes[i], activation = activation[x], \n",
    "                                             algorithm = 'gradient_descent', max_iters = 5000, \n",
    "                                             bias = True, is_classifier = True, learning_rate = learning_rate[y], \n",
    "                                             early_stopping = True, clip_max = 5, max_attempts = 100, \n",
    "                                             random_state = 42, curve = True, restarts = 0)\n",
    "            val, train = strat_kfold_ann(nn_model_base, num_folds=5, x_train = x_train_w, y_train = y_train_w, dataset = 'wine', epochs = 5)\n",
    "            print('nodes: ', nodes[i], 'activations: ', activation[x], 'LR: ', learning_rate[y], 'validation recall: ', val , 'training recall: ', train)\n",
    "            end_time = time.time()\n",
    "            wall_clock_time = end_time - start_time  \n",
    "            print(wall_clock_time)\n",
    "            \n",
    "# Note: Grid Search optimal parameters (layers: 64 nodes x 4 layers, LR: 0.0001), tanh Activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5d58dea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes:  [64, 64, 64] activations:  sigmoid LR:  0.0001 validation recall:  0.8246428571428572 training recall:  0.8432142857142857\n",
      "22.72920274734497\n",
      "nodes:  [64, 64, 64] activations:  sigmoid LR:  1e-05 validation recall:  0.7589285714285714 training recall:  0.7642857142857142\n",
      "22.578039169311523\n",
      "nodes:  [64, 64, 64] activations:  sigmoid LR:  1e-06 validation recall:  0.685 training recall:  0.6828571428571428\n",
      "22.131962299346924\n",
      "nodes:  [64, 64, 64] activations:  relu LR:  0.0001 validation recall:  0.5625 training recall:  0.615\n",
      "27.95182466506958\n",
      "nodes:  [64, 64, 64] activations:  relu LR:  1e-05 validation recall:  0.8403571428571428 training recall:  0.9971428571428571\n",
      "29.940595865249634\n",
      "nodes:  [64, 64, 64] activations:  relu LR:  1e-06 validation recall:  0.8175 training recall:  0.925\n",
      "27.5413236618042\n",
      "nodes:  [64, 64, 64] activations:  tanh LR:  0.0001 validation recall:  0.8689285714285715 training recall:  1.0\n",
      "17.08881187438965\n",
      "nodes:  [64, 64, 64] activations:  tanh LR:  1e-05 validation recall:  0.8157142857142857 training recall:  0.9425\n",
      "18.35967493057251\n",
      "nodes:  [64, 64, 64] activations:  tanh LR:  1e-06 validation recall:  0.7424999999999999 training recall:  0.7817857142857143\n",
      "19.240437984466553\n",
      "nodes:  [64, 64, 64, 64] activations:  sigmoid LR:  0.0001 validation recall:  0.8060714285714285 training recall:  0.8035714285714286\n",
      "32.036357164382935\n",
      "nodes:  [64, 64, 64, 64] activations:  sigmoid LR:  1e-05 validation recall:  0.7621428571428571 training recall:  0.7653571428571428\n",
      "31.891104221343994\n",
      "nodes:  [64, 64, 64, 64] activations:  sigmoid LR:  1e-06 validation recall:  0.6885714285714286 training recall:  0.6882142857142857\n",
      "33.84252619743347\n",
      "nodes:  [64, 64, 64, 64] activations:  relu LR:  0.0001 validation recall:  0.5010714285714286 training recall:  0.5010714285714286\n",
      "4.227128267288208\n",
      "nodes:  [64, 64, 64, 64] activations:  relu LR:  1e-05 validation recall:  0.8067857142857142 training recall:  0.9896428571428572\n",
      "43.13394856452942\n",
      "nodes:  [64, 64, 64, 64] activations:  relu LR:  1e-06 validation recall:  0.8260714285714286 training recall:  0.9871428571428571\n",
      "35.59005880355835\n",
      "nodes:  [64, 64, 64, 64] activations:  tanh LR:  0.0001 validation recall:  0.8628571428571429 training recall:  1.0\n",
      "21.729556798934937\n",
      "nodes:  [64, 64, 64, 64] activations:  tanh LR:  1e-05 validation recall:  0.8171428571428572 training recall:  0.9867857142857143\n",
      "22.426385641098022\n",
      "nodes:  [64, 64, 64, 64] activations:  tanh LR:  1e-06 validation recall:  0.7510714285714286 training recall:  0.8414285714285714\n",
      "22.473429679870605\n",
      "nodes:  [128, 128, 128] activations:  sigmoid LR:  0.0001 validation recall:  0.8335714285714286 training recall:  0.8489285714285715\n",
      "82.87890481948853\n",
      "nodes:  [128, 128, 128] activations:  sigmoid LR:  1e-05 validation recall:  0.7928571428571429 training recall:  0.8064285714285715\n",
      "80.69282078742981\n",
      "nodes:  [128, 128, 128] activations:  sigmoid LR:  1e-06 validation recall:  0.7371428571428571 training recall:  0.7492857142857143\n",
      "79.72134280204773\n",
      "nodes:  [128, 128, 128] activations:  relu LR:  0.0001 validation recall:  0.5028571428571429 training recall:  0.5021428571428571\n",
      "7.892058849334717\n",
      "nodes:  [128, 128, 128] activations:  relu LR:  1e-05 validation recall:  0.8382142857142858 training recall:  1.0\n",
      "79.27795624732971\n",
      "nodes:  [128, 128, 128] activations:  relu LR:  1e-06 validation recall:  0.8478571428571429 training recall:  0.9878571428571429\n",
      "77.87855458259583\n",
      "nodes:  [128, 128, 128] activations:  tanh LR:  0.0001 validation recall:  0.8489285714285715 training recall:  1.0\n",
      "53.33287501335144\n",
      "nodes:  [128, 128, 128] activations:  tanh LR:  1e-05 validation recall:  0.8207142857142857 training recall:  0.9992857142857143\n",
      "52.15689301490784\n",
      "nodes:  [128, 128, 128] activations:  tanh LR:  1e-06 validation recall:  0.7578571428571428 training recall:  0.8678571428571429\n",
      "51.5494601726532\n",
      "nodes:  [128, 128, 128, 128] activations:  sigmoid LR:  0.0001 validation recall:  0.8064285714285715 training recall:  0.8157142857142857\n",
      "110.66885089874268\n",
      "nodes:  [128, 128, 128, 128] activations:  sigmoid LR:  1e-05 validation recall:  0.8014285714285714 training recall:  0.8167857142857142\n",
      "109.8144006729126\n",
      "nodes:  [128, 128, 128, 128] activations:  sigmoid LR:  1e-06 validation recall:  0.7585714285714286 training recall:  0.7642857142857142\n",
      "106.2574315071106\n",
      "nodes:  [128, 128, 128, 128] activations:  relu LR:  0.0001 validation recall:  0.5017857142857143 training recall:  0.5010714285714286\n",
      "10.92520523071289\n",
      "nodes:  [128, 128, 128, 128] activations:  relu LR:  1e-05 validation recall:  0.49857142857142855 training recall:  0.5214285714285715\n",
      "51.41418719291687\n",
      "nodes:  [128, 128, 128, 128] activations:  relu LR:  1e-06 validation recall:  0.8396428571428571 training recall:  1.0\n",
      "108.4764461517334\n",
      "nodes:  [128, 128, 128, 128] activations:  tanh LR:  0.0001 validation recall:  0.8596428571428572 training recall:  1.0\n",
      "72.97416543960571\n",
      "nodes:  [128, 128, 128, 128] activations:  tanh LR:  1e-05 validation recall:  0.8232142857142857 training recall:  1.0\n",
      "72.47779393196106\n",
      "nodes:  [128, 128, 128, 128] activations:  tanh LR:  1e-06 validation recall:  0.785 training recall:  0.9667857142857142\n",
      "69.00381445884705\n"
     ]
    }
   ],
   "source": [
    "nodes = [[64,64,64],[64,64,64,64],[128,128,128],[128,128,128,128]]\n",
    "activation = ['sigmoid','relu','tanh']\n",
    "learning_rate = [0.0001,0.00001, 0.000001]\n",
    "\n",
    "for i in range(0,len(nodes)):\n",
    "    for x in range(0,len(activation)):\n",
    "        for y in range(0,len(learning_rate)):\n",
    "            # Base Case\n",
    "            start_time = time.time()\n",
    "            nn_model_base = mlrose_hiive.NeuralNetwork(hidden_nodes = nodes[i], activation = activation[x], \n",
    "                                             algorithm = 'gradient_descent', max_iters = 1000, \n",
    "                                             bias = True, is_classifier = True, learning_rate = learning_rate[y], \n",
    "                                             early_stopping = True, clip_max = 5, max_attempts = 100, \n",
    "                                             random_state = 42, curve = True, restarts = 0)\n",
    "            val, train = strat_kfold_ann(nn_model_base, num_folds=2, x_train = x_train_a, y_train = y_train_a, dataset = 'apple', epochs = 5)\n",
    "            print('nodes: ', nodes[i], 'activations: ', activation[x], 'LR: ', learning_rate[y], 'validation recall: ', val , 'training recall: ', train)\n",
    "            end_time = time.time()\n",
    "            wall_clock_time = end_time - start_time  \n",
    "            print(wall_clock_time)\n",
    "            \n",
    "# Optimal Hyperparameter: tanh, LR: 0.0001, [64,64,64], validation recall: 0.8689285714285715 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "da5fc6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.76903343200684\n",
      "[-2.55695179e+00 -1.70995482e+00 -1.21885183e+00 ... -4.47305910e-04\n",
      " -4.47171532e-04 -4.47037232e-04]\n"
     ]
    }
   ],
   "source": [
    "# Base Case Curve\n",
    "start_time = time.time()\n",
    "nn_model_base_curve = mlrose_hiive.NeuralNetwork(hidden_nodes = [64,64,64], activation = 'tanh', \n",
    "                                 algorithm = 'gradient_descent', max_iters = 5000, \n",
    "                                 bias = True, is_classifier = True, learning_rate = 0.0001, \n",
    "                                 early_stopping = True, clip_max = 5, max_attempts = 100, \n",
    "                                 random_state = 42, curve = True, restarts = 0)\n",
    "val, train = strat_kfold_ann(nn_model_base_curve, num_folds=2, x_train = x_train_a, y_train = y_train_a, dataset = 'apple', epochs = 5)\n",
    "# print('nodes: ', nodes[i], 'activations: ', activation[x], 'LR: ', learning_rate[y], 'validation recall: ', val , 'training recall: ', train)\n",
    "end_time = time.time()\n",
    "wall_clock_time = end_time - start_time  \n",
    "print(wall_clock_time)\n",
    "\n",
    "print(nn_model_base_curve.fitness_curve)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c3d44b",
   "metadata": {},
   "source": [
    "# Optimal Parameters for RHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2c802ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restarts:  0 validation recall:  0.1266033921389202 training recall:  0.12660436897528168 wall clock time:  230.45974802970886\n",
      "restarts:  1 validation recall:  0.11258879359877423 training recall:  0.11377170400991529 wall clock time:  464.42892384529114\n",
      "restarts:  2 validation recall:  0.1703696452512183 training recall:  0.17036222166735404 wall clock time:  646.4845159053802\n",
      "restarts:  3 validation recall:  0.21674150369219639 training recall:  0.21674427286963197 wall clock time:  818.9746201038361\n",
      "restarts:  4 validation recall:  0.21674150369219639 training recall:  0.21674427286963197 wall clock time:  1144.6038391590118\n",
      "restarts:  5 validation recall:  0.21674150369219639 training recall:  0.21674427286963197 wall clock time:  1262.69962143898\n",
      "restarts:  6 validation recall:  0.21674150369219639 training recall:  0.21674427286963197 wall clock time:  2311.00888466835\n",
      "restarts:  7 validation recall:  0.21674150369219639 training recall:  0.21674427286963197 wall clock time:  2913.4501373767853\n",
      "restarts:  8 validation recall:  0.21674150369219639 training recall:  0.21674427286963197 wall clock time:  2859.189551591873\n",
      "restarts:  9 validation recall:  0.1446860037028367 training recall:  0.14469048327511208 wall clock time:  7259.233862876892\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    start_time = time.time()\n",
    "    nn_model_base = mlrose_hiive.NeuralNetwork(hidden_nodes = [64,64,64,64], activation = 'tanh', \n",
    "                                                 algorithm = 'random_hill_climb', max_iters = 5000, \n",
    "                                                 bias = True, is_classifier = True, learning_rate = 0.0001, \n",
    "                                                 early_stopping = True, clip_max = 5, max_attempts = 100, \n",
    "                                                 random_state = 42, curve = True, restarts = i)\n",
    "    val, train = strat_kfold_ann(nn_model_base, num_folds=5, x_train = x_train_a, y_train = y_train_a, dataset = 'wine', epochs = 5)\n",
    "    end_time = time.time()\n",
    "    wall_clock_time = end_time - start_time\n",
    "    print('restarts: ', i, 'validation recall: ', val , 'training recall: ', train,  'wall clock time: ', wall_clock_time)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b8b690a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restarts:  0 validation recall:  0.4692857142857143 training recall:  0.4692857142857143 wall clock time:  5.086902141571045\n",
      "restarts:  1 validation recall:  0.5517857142857143 training recall:  0.5517857142857143 wall clock time:  9.947561025619507\n",
      "restarts:  2 validation recall:  0.5517857142857143 training recall:  0.5517857142857143 wall clock time:  17.079425573349\n",
      "restarts:  3 validation recall:  0.5728571428571428 training recall:  0.5728571428571428 wall clock time:  22.167468786239624\n",
      "restarts:  4 validation recall:  0.5057142857142857 training recall:  0.5157142857142857 wall clock time:  26.564356088638306\n",
      "restarts:  5 validation recall:  0.49 training recall:  0.49 wall clock time:  32.13147258758545\n",
      "restarts:  6 validation recall:  0.49 training recall:  0.49 wall clock time:  37.65542149543762\n",
      "restarts:  7 validation recall:  0.49 training recall:  0.49 wall clock time:  45.68847918510437\n",
      "restarts:  8 validation recall:  0.49 training recall:  0.49 wall clock time:  69.04184055328369\n",
      "restarts:  9 validation recall:  0.49 training recall:  0.49 wall clock time:  73.55516171455383\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    start_time = time.time()\n",
    "    nn_model_base = mlrose_hiive.NeuralNetwork(hidden_nodes = [64,64,64], activation = 'tanh', \n",
    "                                                 algorithm = 'random_hill_climb', max_iters = 1000, \n",
    "                                                 bias = True, is_classifier = True, learning_rate = 0.0001, \n",
    "                                                 early_stopping = True, clip_max = 5, max_attempts = 100, \n",
    "                                                 random_state = 42, curve = True, restarts = i)\n",
    "    val, train = strat_kfold_ann(nn_model_base, num_folds=2, x_train = x_train_a, y_train = y_train_a, dataset = 'apple', epochs = 5)\n",
    "    end_time = time.time()\n",
    "    wall_clock_time = end_time - start_time\n",
    "    print('restarts: ', i, 'validation recall: ', val , 'training recall: ', train,  'wall clock time: ', wall_clock_time)\n",
    "\n",
    "# Optimal Hyperparameter: tanh, LR: 0.0001, [64,64,64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "27bc0685",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restarts:  0 validation recall:  0.52 training recall:  0.5203571428571429 wall clock time:  13.840313196182251\n",
      "restarts:  1 validation recall:  0.5728571428571428 training recall:  0.5728571428571428 wall clock time:  27.132741689682007\n",
      "restarts:  2 validation recall:  0.4642857142857143 training recall:  0.4642857142857143 wall clock time:  41.4157178401947\n",
      "restarts:  3 validation recall:  0.5249999999999999 training recall:  0.5249999999999999 wall clock time:  59.68016815185547\n",
      "restarts:  4 validation recall:  0.64 training recall:  0.6403571428571428 wall clock time:  72.08869814872742\n",
      "restarts:  5 validation recall:  0.5457142857142857 training recall:  0.5457142857142857 wall clock time:  87.09942030906677\n",
      "restarts:  6 validation recall:  0.5017857142857143 training recall:  0.5021428571428572 wall clock time:  101.2013807296753\n",
      "restarts:  7 validation recall:  0.5264285714285715 training recall:  0.5275 wall clock time:  116.76694440841675\n",
      "restarts:  8 validation recall:  0.5564285714285715 training recall:  0.5564285714285714 wall clock time:  140.88182067871094\n",
      "restarts:  9 validation recall:  0.4692857142857143 training recall:  0.4692857142857143 wall clock time:  146.4584081172943\n"
     ]
    }
   ],
   "source": [
    "ftcurve = []\n",
    "for i in range(0,10):\n",
    "    start_time = time.time()\n",
    "    nn_model_base = mlrose_hiive.NeuralNetwork(hidden_nodes = [64,64,64], activation = 'tanh', \n",
    "                                                 algorithm = 'random_hill_climb', max_iters = 500+500*i, \n",
    "                                                 bias = True, is_classifier = True, learning_rate = 0.0001, \n",
    "                                                 early_stopping = True, clip_max = 5, max_attempts = 100, \n",
    "                                                 random_state = 42, curve = True, restarts = 3)\n",
    "    val, train = strat_kfold_ann(nn_model_base, num_folds=2, x_train = x_train_a, y_train = y_train_a, dataset = 'apple', epochs = 5)\n",
    "    end_time = time.time()\n",
    "    wall_clock_time = end_time - start_time\n",
    "    ftcurve.append(nn_model_base.fitness_curve)\n",
    "    print('restarts: ', i, 'validation recall: ', val , 'training recall: ', train,  'wall clock time: ', wall_clock_time)\n",
    "\n",
    "# Optimal Hyperparameter: tanh, LR: 0.0001, [64,64,64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2d75f115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[2.69845353e+00, 2.28100000e+03],\n",
      "       [2.69845346e+00, 2.28300000e+03],\n",
      "       [2.69845346e+00, 2.28400000e+03],\n",
      "       [2.69845028e+00, 2.28600000e+03],\n",
      "       [2.69844792e+00, 2.28800000e+03],\n",
      "       [2.69844744e+00, 2.29000000e+03],\n",
      "       [2.69844605e+00, 2.29200000e+03],\n",
      "       [2.69844605e+00, 2.29300000e+03],\n",
      "       [2.69844588e+00, 2.29500000e+03],\n",
      "       [2.69844588e+00, 2.29600000e+03],\n",
      "       [2.69844526e+00, 2.29800000e+03],\n",
      "       [2.69844300e+00, 2.30000000e+03],\n",
      "       [2.69844217e+00, 2.30200000e+03],\n",
      "       [2.69844131e+00, 2.30400000e+03],\n",
      "       [2.69844131e+00, 2.30500000e+03],\n",
      "       [2.69844131e+00, 2.30600000e+03],\n",
      "       [2.69844023e+00, 2.30800000e+03],\n",
      "       [2.69844011e+00, 2.31000000e+03],\n",
      "       [2.69843989e+00, 2.31200000e+03],\n",
      "       [2.69843272e+00, 2.31400000e+03],\n",
      "       [2.69843037e+00, 2.31600000e+03],\n",
      "       [2.69842918e+00, 2.31800000e+03],\n",
      "       [2.69842860e+00, 2.32000000e+03],\n",
      "       [2.69842701e+00, 2.32200000e+03],\n",
      "       [2.69842497e+00, 2.32400000e+03],\n",
      "       [2.69842497e+00, 2.32500000e+03],\n",
      "       [2.69842456e+00, 2.32700000e+03],\n",
      "       [2.69842412e+00, 2.32900000e+03],\n",
      "       [2.69842330e+00, 2.33100000e+03],\n",
      "       [2.69842330e+00, 2.33200000e+03],\n",
      "       [2.69842330e+00, 2.33300000e+03],\n",
      "       [2.69842181e+00, 2.33500000e+03],\n",
      "       [2.69841124e+00, 2.33700000e+03],\n",
      "       [2.69841124e+00, 2.33800000e+03],\n",
      "       [2.69841124e+00, 2.33900000e+03],\n",
      "       [2.69840863e+00, 2.34100000e+03],\n",
      "       [2.69840831e+00, 2.34300000e+03],\n",
      "       [2.69840831e+00, 2.34400000e+03],\n",
      "       [2.69840753e+00, 2.34600000e+03],\n",
      "       [2.69840702e+00, 2.34800000e+03],\n",
      "       [2.69840610e+00, 2.35000000e+03],\n",
      "       [2.69840610e+00, 2.35100000e+03],\n",
      "       [2.69840610e+00, 2.35200000e+03],\n",
      "       [2.69840610e+00, 2.35300000e+03],\n",
      "       [2.69839467e+00, 2.35500000e+03],\n",
      "       [2.69839467e+00, 2.35600000e+03],\n",
      "       [2.69839467e+00, 2.35700000e+03],\n",
      "       [2.69839467e+00, 2.35800000e+03],\n",
      "       [2.69839419e+00, 2.36000000e+03],\n",
      "       [2.69839399e+00, 2.36200000e+03],\n",
      "       [2.69839399e+00, 2.36300000e+03],\n",
      "       [2.69839309e+00, 2.36500000e+03],\n",
      "       [2.69839216e+00, 2.36700000e+03],\n",
      "       [2.69838930e+00, 2.36900000e+03],\n",
      "       [2.69838930e+00, 2.37000000e+03],\n",
      "       [2.69838854e+00, 2.37200000e+03],\n",
      "       [2.69838854e+00, 2.37300000e+03],\n",
      "       [2.69838854e+00, 2.37400000e+03],\n",
      "       [2.69838754e+00, 2.37600000e+03],\n",
      "       [2.69837558e+00, 2.37800000e+03],\n",
      "       [2.69836946e+00, 2.38000000e+03],\n",
      "       [2.69836946e+00, 2.38100000e+03],\n",
      "       [2.69836946e+00, 2.38200000e+03],\n",
      "       [2.69836946e+00, 2.38300000e+03],\n",
      "       [2.69836946e+00, 2.38400000e+03],\n",
      "       [2.69836946e+00, 2.38500000e+03],\n",
      "       [2.69836946e+00, 2.38600000e+03],\n",
      "       [2.69836560e+00, 2.38800000e+03],\n",
      "       [2.69836560e+00, 2.38900000e+03],\n",
      "       [2.69836560e+00, 2.39000000e+03],\n",
      "       [2.69836534e+00, 2.39200000e+03],\n",
      "       [2.69836347e+00, 2.39400000e+03],\n",
      "       [2.69836283e+00, 2.39600000e+03],\n",
      "       [2.69836283e+00, 2.39700000e+03],\n",
      "       [2.69835884e+00, 2.39900000e+03],\n",
      "       [2.69835671e+00, 2.40100000e+03],\n",
      "       [2.69835671e+00, 2.40200000e+03],\n",
      "       [2.69835632e+00, 2.40400000e+03],\n",
      "       [2.69835632e+00, 2.40500000e+03],\n",
      "       [2.69835632e+00, 2.40600000e+03],\n",
      "       [2.69835632e+00, 2.40700000e+03],\n",
      "       [2.69835296e+00, 2.40900000e+03],\n",
      "       [2.69835296e+00, 2.41000000e+03],\n",
      "       [2.69835294e+00, 2.41200000e+03],\n",
      "       [2.69835294e+00, 2.41300000e+03],\n",
      "       [2.69835267e+00, 2.41500000e+03],\n",
      "       [2.69835242e+00, 2.41700000e+03],\n",
      "       [2.69835155e+00, 2.41900000e+03],\n",
      "       [2.69835155e+00, 2.42000000e+03],\n",
      "       [2.69835122e+00, 2.42200000e+03],\n",
      "       [2.69835034e+00, 2.42400000e+03],\n",
      "       [2.69835034e+00, 2.42500000e+03],\n",
      "       [2.69835034e+00, 2.42600000e+03],\n",
      "       [2.69834945e+00, 2.42800000e+03],\n",
      "       [2.69834945e+00, 2.42900000e+03],\n",
      "       [2.69834656e+00, 2.43100000e+03],\n",
      "       [2.69834656e+00, 2.43200000e+03],\n",
      "       [2.69834656e+00, 2.43300000e+03],\n",
      "       [2.69834656e+00, 2.43400000e+03],\n",
      "       [2.69834540e+00, 2.43600000e+03],\n",
      "       [2.69834432e+00, 2.43800000e+03],\n",
      "       [2.69834432e+00, 2.43900000e+03],\n",
      "       [2.69834400e+00, 2.44100000e+03],\n",
      "       [2.69834338e+00, 2.44300000e+03],\n",
      "       [2.69834338e+00, 2.44400000e+03],\n",
      "       [2.69833892e+00, 2.44600000e+03],\n",
      "       [2.69833892e+00, 2.44700000e+03],\n",
      "       [2.69833892e+00, 2.44800000e+03],\n",
      "       [2.69833892e+00, 2.44900000e+03],\n",
      "       [2.69833892e+00, 2.45000000e+03],\n",
      "       [2.69833892e+00, 2.45100000e+03],\n",
      "       [2.69833842e+00, 2.45300000e+03],\n",
      "       [2.69833823e+00, 2.45500000e+03],\n",
      "       [2.69833823e+00, 2.45600000e+03],\n",
      "       [2.69833596e+00, 2.45800000e+03],\n",
      "       [2.69833520e+00, 2.46000000e+03],\n",
      "       [2.69833519e+00, 2.46200000e+03],\n",
      "       [2.69833294e+00, 2.46400000e+03],\n",
      "       [2.69833294e+00, 2.46500000e+03],\n",
      "       [2.69833255e+00, 2.46700000e+03],\n",
      "       [2.69833255e+00, 2.46800000e+03],\n",
      "       [2.69833255e+00, 2.46900000e+03],\n",
      "       [2.69833253e+00, 2.47100000e+03],\n",
      "       [2.69833253e+00, 2.47200000e+03],\n",
      "       [2.69833253e+00, 2.47300000e+03],\n",
      "       [2.69833216e+00, 2.47500000e+03],\n",
      "       [2.69833216e+00, 2.47600000e+03],\n",
      "       [2.69832617e+00, 2.47800000e+03],\n",
      "       [2.69832617e+00, 2.47900000e+03],\n",
      "       [2.69832274e+00, 2.48100000e+03],\n",
      "       [2.69832204e+00, 2.48300000e+03],\n",
      "       [2.69832204e+00, 2.48400000e+03],\n",
      "       [2.69832204e+00, 2.48500000e+03],\n",
      "       [2.69831951e+00, 2.48700000e+03],\n",
      "       [2.69831936e+00, 2.48900000e+03],\n",
      "       [2.69831918e+00, 2.49100000e+03],\n",
      "       [2.69831500e+00, 2.49300000e+03],\n",
      "       [2.69831500e+00, 2.49400000e+03],\n",
      "       [2.69831500e+00, 2.49500000e+03],\n",
      "       [2.69831500e+00, 2.49600000e+03],\n",
      "       [2.69831500e+00, 2.49700000e+03],\n",
      "       [2.69831500e+00, 2.49800000e+03],\n",
      "       [2.69831500e+00, 2.49900000e+03],\n",
      "       [2.69831500e+00, 2.50000000e+03],\n",
      "       [2.69831491e+00, 2.50200000e+03],\n",
      "       [2.69831491e+00, 2.50300000e+03],\n",
      "       [2.69831491e+00, 2.50400000e+03],\n",
      "       [2.69831491e+00, 2.50500000e+03],\n",
      "       [2.69831491e+00, 2.50600000e+03],\n",
      "       [2.69831491e+00, 2.50700000e+03],\n",
      "       [2.69831228e+00, 2.50900000e+03],\n",
      "       [2.69830989e+00, 2.51100000e+03],\n",
      "       [2.69830989e+00, 2.51200000e+03],\n",
      "       [2.69830989e+00, 2.51300000e+03],\n",
      "       [2.69830429e+00, 2.51500000e+03],\n",
      "       [2.69830429e+00, 2.51600000e+03],\n",
      "       [2.69830382e+00, 2.51800000e+03],\n",
      "       [2.69830382e+00, 2.51900000e+03],\n",
      "       [2.69830267e+00, 2.52100000e+03],\n",
      "       [2.69829778e+00, 2.52300000e+03],\n",
      "       [2.69829588e+00, 2.52500000e+03],\n",
      "       [2.69829588e+00, 2.52600000e+03],\n",
      "       [2.69829271e+00, 2.52800000e+03],\n",
      "       [2.69829271e+00, 2.52900000e+03],\n",
      "       [2.69829214e+00, 2.53100000e+03],\n",
      "       [2.69827665e+00, 2.53300000e+03],\n",
      "       [2.69827620e+00, 2.53500000e+03],\n",
      "       [2.69827620e+00, 2.53600000e+03],\n",
      "       [2.69827290e+00, 2.53800000e+03],\n",
      "       [2.69827143e+00, 2.54000000e+03],\n",
      "       [2.69826692e+00, 2.54200000e+03],\n",
      "       [2.69826692e+00, 2.54300000e+03],\n",
      "       [2.69826692e+00, 2.54400000e+03],\n",
      "       [2.69826692e+00, 2.54500000e+03],\n",
      "       [2.69826687e+00, 2.54700000e+03],\n",
      "       [2.69826214e+00, 2.54900000e+03],\n",
      "       [2.69826214e+00, 2.55000000e+03],\n",
      "       [2.69826214e+00, 2.55100000e+03],\n",
      "       [2.69825941e+00, 2.55300000e+03],\n",
      "       [2.69825941e+00, 2.55400000e+03],\n",
      "       [2.69825589e+00, 2.55600000e+03],\n",
      "       [2.69825589e+00, 2.55700000e+03],\n",
      "       [2.69825537e+00, 2.55900000e+03],\n",
      "       [2.69825537e+00, 2.56000000e+03],\n",
      "       [2.69825537e+00, 2.56100000e+03],\n",
      "       [2.69825429e+00, 2.56300000e+03],\n",
      "       [2.69825032e+00, 2.56500000e+03],\n",
      "       [2.69824886e+00, 2.56700000e+03],\n",
      "       [2.69824886e+00, 2.56800000e+03],\n",
      "       [2.69824886e+00, 2.56900000e+03],\n",
      "       [2.69824886e+00, 2.57000000e+03],\n",
      "       [2.69824886e+00, 2.57100000e+03],\n",
      "       [2.69824420e+00, 2.57300000e+03],\n",
      "       [2.69824287e+00, 2.57500000e+03],\n",
      "       [2.69824151e+00, 2.57700000e+03],\n",
      "       [2.69824151e+00, 2.57800000e+03],\n",
      "       [2.69823254e+00, 2.58000000e+03],\n",
      "       [2.69823254e+00, 2.58100000e+03],\n",
      "       [2.69823254e+00, 2.58200000e+03],\n",
      "       [2.69823254e+00, 2.58300000e+03],\n",
      "       [2.69823224e+00, 2.58500000e+03],\n",
      "       [2.69823224e+00, 2.58600000e+03],\n",
      "       [2.69823224e+00, 2.58700000e+03],\n",
      "       [2.69823032e+00, 2.58900000e+03],\n",
      "       [2.69823024e+00, 2.59100000e+03],\n",
      "       [2.69822648e+00, 2.59300000e+03],\n",
      "       [2.69822648e+00, 2.59400000e+03],\n",
      "       [2.69822266e+00, 2.59600000e+03],\n",
      "       [2.69822266e+00, 2.59700000e+03],\n",
      "       [2.69822266e+00, 2.59800000e+03],\n",
      "       [2.69822216e+00, 2.60000000e+03],\n",
      "       [2.69822156e+00, 2.60200000e+03],\n",
      "       [2.69822156e+00, 2.60300000e+03],\n",
      "       [2.69821988e+00, 2.60500000e+03],\n",
      "       [2.69821899e+00, 2.60700000e+03],\n",
      "       [2.69821899e+00, 2.60800000e+03],\n",
      "       [2.69820951e+00, 2.61000000e+03],\n",
      "       [2.69820951e+00, 2.61100000e+03],\n",
      "       [2.69820882e+00, 2.61300000e+03],\n",
      "       [2.69820882e+00, 2.61400000e+03],\n",
      "       [2.69820111e+00, 2.61600000e+03],\n",
      "       [2.69819631e+00, 2.61800000e+03],\n",
      "       [2.69819583e+00, 2.62000000e+03],\n",
      "       [2.69819579e+00, 2.62200000e+03],\n",
      "       [2.69819579e+00, 2.62300000e+03],\n",
      "       [2.69819579e+00, 2.62400000e+03],\n",
      "       [2.69819166e+00, 2.62600000e+03],\n",
      "       [2.69819128e+00, 2.62800000e+03],\n",
      "       [2.69819071e+00, 2.63000000e+03],\n",
      "       [2.69818653e+00, 2.63200000e+03],\n",
      "       [2.69818634e+00, 2.63400000e+03],\n",
      "       [2.69818612e+00, 2.63600000e+03],\n",
      "       [2.69818572e+00, 2.63800000e+03],\n",
      "       [2.69818554e+00, 2.64000000e+03],\n",
      "       [2.69818477e+00, 2.64200000e+03],\n",
      "       [2.69818388e+00, 2.64400000e+03],\n",
      "       [2.69818388e+00, 2.64500000e+03],\n",
      "       [2.69818291e+00, 2.64700000e+03],\n",
      "       [2.69818233e+00, 2.64900000e+03],\n",
      "       [2.69818216e+00, 2.65100000e+03],\n",
      "       [2.69818164e+00, 2.65300000e+03],\n",
      "       [2.69818120e+00, 2.65500000e+03],\n",
      "       [2.69817972e+00, 2.65700000e+03],\n",
      "       [2.69817972e+00, 2.65800000e+03],\n",
      "       [2.69817972e+00, 2.65900000e+03],\n",
      "       [2.69817893e+00, 2.66100000e+03],\n",
      "       [2.69817709e+00, 2.66300000e+03],\n",
      "       [2.69817705e+00, 2.66500000e+03],\n",
      "       [2.69817705e+00, 2.66600000e+03],\n",
      "       [2.69817705e+00, 2.66700000e+03],\n",
      "       [2.69817705e+00, 2.66800000e+03],\n",
      "       [2.69817542e+00, 2.67000000e+03],\n",
      "       [2.69817542e+00, 2.67100000e+03],\n",
      "       [2.69817413e+00, 2.67300000e+03],\n",
      "       [2.69817413e+00, 2.67400000e+03],\n",
      "       [2.69817062e+00, 2.67600000e+03],\n",
      "       [2.69817062e+00, 2.67700000e+03],\n",
      "       [2.69817051e+00, 2.67900000e+03],\n",
      "       [2.69817051e+00, 2.68000000e+03],\n",
      "       [2.69817051e+00, 2.68100000e+03],\n",
      "       [2.69817051e+00, 2.68200000e+03],\n",
      "       [2.69816583e+00, 2.68400000e+03],\n",
      "       [2.69816396e+00, 2.68600000e+03],\n",
      "       [2.69816396e+00, 2.68700000e+03],\n",
      "       [2.69816229e+00, 2.68900000e+03],\n",
      "       [2.69816229e+00, 2.69000000e+03],\n",
      "       [2.69815256e+00, 2.69200000e+03],\n",
      "       [2.69815256e+00, 2.69300000e+03],\n",
      "       [2.69814901e+00, 2.69500000e+03],\n",
      "       [2.69814866e+00, 2.69700000e+03],\n",
      "       [2.69814846e+00, 2.69900000e+03],\n",
      "       [2.69814846e+00, 2.70000000e+03],\n",
      "       [2.69814808e+00, 2.70200000e+03],\n",
      "       [2.69814808e+00, 2.70300000e+03],\n",
      "       [2.69814808e+00, 2.70400000e+03],\n",
      "       [2.69814808e+00, 2.70500000e+03],\n",
      "       [2.69814808e+00, 2.70600000e+03],\n",
      "       [2.69814788e+00, 2.70800000e+03],\n",
      "       [2.69814788e+00, 2.70900000e+03],\n",
      "       [2.69814501e+00, 2.71100000e+03],\n",
      "       [2.69814498e+00, 2.71300000e+03],\n",
      "       [2.69814325e+00, 2.71500000e+03],\n",
      "       [2.69814325e+00, 2.71600000e+03],\n",
      "       [2.69814200e+00, 2.71800000e+03],\n",
      "       [2.69814016e+00, 2.72000000e+03],\n",
      "       [2.69814016e+00, 2.72100000e+03],\n",
      "       [2.69814016e+00, 2.72200000e+03],\n",
      "       [2.69814016e+00, 2.72300000e+03],\n",
      "       [2.69814016e+00, 2.72400000e+03],\n",
      "       [2.69814016e+00, 2.72500000e+03],\n",
      "       [2.69814016e+00, 2.72600000e+03],\n",
      "       [2.69814016e+00, 2.72700000e+03],\n",
      "       [2.69814016e+00, 2.72800000e+03],\n",
      "       [2.69813948e+00, 2.73000000e+03],\n",
      "       [2.69813877e+00, 2.73200000e+03],\n",
      "       [2.69813810e+00, 2.73400000e+03],\n",
      "       [2.69813810e+00, 2.73500000e+03],\n",
      "       [2.69813564e+00, 2.73700000e+03],\n",
      "       [2.69813564e+00, 2.73800000e+03],\n",
      "       [2.69813564e+00, 2.73900000e+03],\n",
      "       [2.69813564e+00, 2.74000000e+03],\n",
      "       [2.69813564e+00, 2.74100000e+03],\n",
      "       [2.69813564e+00, 2.74200000e+03],\n",
      "       [2.69813541e+00, 2.74400000e+03],\n",
      "       [2.69813541e+00, 2.74500000e+03],\n",
      "       [2.69813539e+00, 2.74700000e+03],\n",
      "       [2.69813443e+00, 2.74900000e+03],\n",
      "       [2.69813443e+00, 2.75000000e+03],\n",
      "       [2.69813443e+00, 2.75100000e+03],\n",
      "       [2.69813443e+00, 2.75300000e+03],\n",
      "       [2.69813443e+00, 2.75400000e+03],\n",
      "       [2.69813443e+00, 2.75500000e+03],\n",
      "       [2.69813443e+00, 2.75600000e+03],\n",
      "       [2.69813277e+00, 2.75800000e+03],\n",
      "       [2.69813095e+00, 2.76000000e+03],\n",
      "       [2.69813095e+00, 2.76100000e+03],\n",
      "       [2.69813095e+00, 2.76200000e+03],\n",
      "       [2.69813027e+00, 2.76400000e+03],\n",
      "       [2.69813027e+00, 2.76500000e+03],\n",
      "       [2.69812998e+00, 2.76700000e+03],\n",
      "       [2.69812998e+00, 2.76800000e+03],\n",
      "       [2.69812910e+00, 2.77000000e+03],\n",
      "       [2.69812702e+00, 2.77200000e+03],\n",
      "       [2.69812702e+00, 2.77300000e+03],\n",
      "       [2.69812615e+00, 2.77500000e+03],\n",
      "       [2.69812492e+00, 2.77700000e+03],\n",
      "       [2.69812492e+00, 2.77800000e+03],\n",
      "       [2.69812269e+00, 2.78000000e+03],\n",
      "       [2.69812195e+00, 2.78200000e+03],\n",
      "       [2.69812195e+00, 2.78300000e+03],\n",
      "       [2.69812149e+00, 2.78500000e+03],\n",
      "       [2.69812149e+00, 2.78600000e+03],\n",
      "       [2.69811802e+00, 2.78800000e+03],\n",
      "       [2.69811802e+00, 2.78900000e+03],\n",
      "       [2.69811772e+00, 2.79100000e+03],\n",
      "       [2.69811323e+00, 2.79300000e+03],\n",
      "       [2.69811263e+00, 2.79500000e+03],\n",
      "       [2.69811263e+00, 2.79600000e+03],\n",
      "       [2.69811181e+00, 2.79800000e+03],\n",
      "       [2.69811034e+00, 2.80000000e+03],\n",
      "       [2.69810850e+00, 2.80200000e+03],\n",
      "       [2.69810649e+00, 2.80400000e+03],\n",
      "       [2.69810649e+00, 2.80500000e+03],\n",
      "       [2.69810649e+00, 2.80600000e+03],\n",
      "       [2.69810364e+00, 2.80800000e+03],\n",
      "       [2.69810364e+00, 2.80900000e+03],\n",
      "       [2.69810337e+00, 2.81100000e+03],\n",
      "       [2.69810281e+00, 2.81300000e+03],\n",
      "       [2.69810281e+00, 2.81400000e+03],\n",
      "       [2.69810281e+00, 2.81500000e+03],\n",
      "       [2.69810231e+00, 2.81700000e+03],\n",
      "       [2.69810231e+00, 2.81800000e+03],\n",
      "       [2.69810134e+00, 2.82000000e+03],\n",
      "       [2.69810134e+00, 2.82100000e+03],\n",
      "       [2.69810017e+00, 2.82300000e+03],\n",
      "       [2.69809849e+00, 2.82500000e+03],\n",
      "       [2.69809827e+00, 2.82700000e+03],\n",
      "       [2.69809231e+00, 2.82900000e+03],\n",
      "       [2.69809231e+00, 2.83000000e+03],\n",
      "       [2.69809167e+00, 2.83200000e+03],\n",
      "       [2.69809167e+00, 2.83300000e+03],\n",
      "       [2.69809141e+00, 2.83500000e+03],\n",
      "       [2.69807516e+00, 2.83700000e+03],\n",
      "       [2.69807516e+00, 2.83800000e+03],\n",
      "       [2.69807516e+00, 2.83900000e+03],\n",
      "       [2.69807516e+00, 2.84000000e+03],\n",
      "       [2.69806952e+00, 2.84200000e+03],\n",
      "       [2.69806899e+00, 2.84400000e+03],\n",
      "       [2.69806899e+00, 2.84500000e+03],\n",
      "       [2.69806899e+00, 2.84600000e+03],\n",
      "       [2.69806836e+00, 2.84800000e+03],\n",
      "       [2.69806771e+00, 2.85000000e+03],\n",
      "       [2.69806771e+00, 2.85100000e+03],\n",
      "       [2.69806594e+00, 2.85300000e+03],\n",
      "       [2.69806408e+00, 2.85500000e+03],\n",
      "       [2.69804121e+00, 2.85700000e+03],\n",
      "       [2.69803784e+00, 2.85900000e+03],\n",
      "       [2.69803769e+00, 2.86100000e+03],\n",
      "       [2.69803769e+00, 2.86200000e+03],\n",
      "       [2.69803750e+00, 2.86400000e+03],\n",
      "       [2.69803750e+00, 2.86500000e+03],\n",
      "       [2.69803711e+00, 2.86700000e+03],\n",
      "       [2.69803711e+00, 2.86800000e+03],\n",
      "       [2.69803258e+00, 2.87000000e+03],\n",
      "       [2.69803258e+00, 2.87100000e+03],\n",
      "       [2.69803258e+00, 2.87200000e+03],\n",
      "       [2.69803258e+00, 2.87300000e+03],\n",
      "       [2.69803258e+00, 2.87400000e+03],\n",
      "       [2.69803258e+00, 2.87500000e+03],\n",
      "       [2.69803258e+00, 2.87600000e+03],\n",
      "       [2.69803246e+00, 2.87800000e+03],\n",
      "       [2.69803246e+00, 2.87900000e+03],\n",
      "       [2.69803246e+00, 2.88000000e+03],\n",
      "       [2.69803246e+00, 2.88100000e+03],\n",
      "       [2.69803246e+00, 2.88200000e+03],\n",
      "       [2.69803082e+00, 2.88400000e+03],\n",
      "       [2.69802980e+00, 2.88600000e+03],\n",
      "       [2.69802965e+00, 2.88800000e+03],\n",
      "       [2.69802965e+00, 2.88900000e+03],\n",
      "       [2.69802886e+00, 2.89100000e+03],\n",
      "       [2.69802886e+00, 2.89200000e+03],\n",
      "       [2.69802716e+00, 2.89400000e+03],\n",
      "       [2.69802529e+00, 2.89600000e+03],\n",
      "       [2.69802529e+00, 2.89700000e+03],\n",
      "       [2.69802090e+00, 2.89900000e+03],\n",
      "       [2.69801859e+00, 2.90100000e+03],\n",
      "       [2.69801396e+00, 2.90300000e+03],\n",
      "       [2.69801396e+00, 2.90400000e+03],\n",
      "       [2.69801396e+00, 2.90500000e+03],\n",
      "       [2.69801251e+00, 2.90700000e+03],\n",
      "       [2.69801251e+00, 2.90800000e+03],\n",
      "       [2.69801251e+00, 2.90900000e+03],\n",
      "       [2.69801211e+00, 2.91100000e+03],\n",
      "       [2.69801166e+00, 2.91300000e+03],\n",
      "       [2.69801166e+00, 2.91400000e+03],\n",
      "       [2.69801166e+00, 2.91500000e+03],\n",
      "       [2.69801166e+00, 2.91600000e+03],\n",
      "       [2.69801166e+00, 2.91700000e+03],\n",
      "       [2.69801166e+00, 2.91800000e+03],\n",
      "       [2.69801166e+00, 2.91900000e+03],\n",
      "       [2.69801166e+00, 2.92000000e+03],\n",
      "       [2.69801166e+00, 2.92100000e+03],\n",
      "       [2.69801085e+00, 2.92300000e+03],\n",
      "       [2.69801085e+00, 2.92400000e+03],\n",
      "       [2.69801085e+00, 2.92500000e+03],\n",
      "       [2.69801085e+00, 2.92600000e+03],\n",
      "       [2.69801015e+00, 2.92800000e+03],\n",
      "       [2.69800631e+00, 2.93000000e+03],\n",
      "       [2.69800631e+00, 2.93100000e+03],\n",
      "       [2.69800463e+00, 2.93300000e+03],\n",
      "       [2.69800360e+00, 2.93500000e+03],\n",
      "       [2.69800335e+00, 2.93700000e+03],\n",
      "       [2.69800335e+00, 2.93800000e+03],\n",
      "       [2.69800310e+00, 2.94000000e+03],\n",
      "       [2.69800310e+00, 2.94100000e+03],\n",
      "       [2.69800301e+00, 2.94300000e+03],\n",
      "       [2.69800257e+00, 2.94500000e+03],\n",
      "       [2.69800257e+00, 2.94600000e+03],\n",
      "       [2.69800229e+00, 2.94800000e+03],\n",
      "       [2.69800177e+00, 2.95000000e+03],\n",
      "       [2.69800036e+00, 2.95200000e+03],\n",
      "       [2.69800036e+00, 2.95300000e+03],\n",
      "       [2.69800033e+00, 2.95500000e+03],\n",
      "       [2.69799616e+00, 2.95700000e+03],\n",
      "       [2.69799516e+00, 2.95900000e+03],\n",
      "       [2.69799370e+00, 2.96100000e+03],\n",
      "       [2.69799011e+00, 2.96300000e+03],\n",
      "       [2.69799011e+00, 2.96400000e+03],\n",
      "       [2.69797153e+00, 2.96600000e+03],\n",
      "       [2.69797153e+00, 2.96700000e+03],\n",
      "       [2.69797153e+00, 2.96800000e+03],\n",
      "       [2.69797153e+00, 2.96900000e+03],\n",
      "       [2.69796910e+00, 2.97100000e+03],\n",
      "       [2.69796910e+00, 2.97200000e+03],\n",
      "       [2.69796904e+00, 2.97400000e+03],\n",
      "       [2.69796580e+00, 2.97600000e+03],\n",
      "       [2.69796570e+00, 2.97800000e+03],\n",
      "       [2.69796498e+00, 2.98000000e+03],\n",
      "       [2.69796346e+00, 2.98200000e+03],\n",
      "       [2.69796171e+00, 2.98400000e+03],\n",
      "       [2.69796171e+00, 2.98500000e+03],\n",
      "       [2.69795947e+00, 2.98700000e+03],\n",
      "       [2.69795947e+00, 2.98800000e+03],\n",
      "       [2.69795947e+00, 2.98900000e+03],\n",
      "       [2.69795947e+00, 2.99000000e+03],\n",
      "       [2.69795947e+00, 2.99100000e+03],\n",
      "       [2.69795828e+00, 2.99300000e+03],\n",
      "       [2.69795731e+00, 2.99500000e+03],\n",
      "       [2.69795731e+00, 2.99600000e+03],\n",
      "       [2.69795731e+00, 2.99700000e+03],\n",
      "       [2.69795636e+00, 2.99900000e+03],\n",
      "       [2.69795504e+00, 3.00100000e+03],\n",
      "       [2.69795504e+00, 3.00200000e+03],\n",
      "       [2.69795468e+00, 3.00400000e+03],\n",
      "       [2.69795468e+00, 3.00500000e+03],\n",
      "       [2.69795468e+00, 3.00600000e+03],\n",
      "       [2.69795408e+00, 3.00800000e+03],\n",
      "       [2.69795408e+00, 3.00900000e+03],\n",
      "       [2.69795249e+00, 3.01100000e+03],\n",
      "       [2.69795249e+00, 3.01200000e+03],\n",
      "       [2.69795249e+00, 3.01300000e+03],\n",
      "       [2.69795249e+00, 3.01400000e+03],\n",
      "       [2.69795190e+00, 3.01600000e+03],\n",
      "       [2.69795190e+00, 3.01700000e+03],\n",
      "       [2.69795190e+00, 3.01800000e+03],\n",
      "       [2.69795190e+00, 3.01900000e+03],\n",
      "       [2.69795190e+00, 3.02000000e+03],\n",
      "       [2.69795083e+00, 3.02200000e+03],\n",
      "       [2.69794486e+00, 3.02400000e+03],\n",
      "       [2.69794385e+00, 3.02600000e+03],\n",
      "       [2.69794290e+00, 3.02800000e+03],\n",
      "       [2.69794266e+00, 3.03000000e+03],\n",
      "       [2.69794266e+00, 3.03100000e+03],\n",
      "       [2.69794266e+00, 3.03200000e+03],\n",
      "       [2.69794266e+00, 3.03300000e+03],\n",
      "       [2.69794266e+00, 3.03400000e+03],\n",
      "       [2.69794266e+00, 3.03500000e+03],\n",
      "       [2.69794266e+00, 3.03600000e+03],\n",
      "       [2.69794185e+00, 3.03800000e+03],\n",
      "       [2.69793913e+00, 3.04000000e+03]]), array([[2.08541927e+00, 4.46500000e+03],\n",
      "       [2.08541927e+00, 4.46600000e+03],\n",
      "       [2.08541927e+00, 4.46700000e+03],\n",
      "       ...,\n",
      "       [2.08468431e+00, 5.94600000e+03],\n",
      "       [2.08468431e+00, 5.94700000e+03],\n",
      "       [2.08468431e+00, 5.94800000e+03]]), array([[1.72007047e+00, 6.79000000e+03],\n",
      "       [1.72007002e+00, 6.79200000e+03],\n",
      "       [1.72007002e+00, 6.79300000e+03],\n",
      "       ...,\n",
      "       [1.71934557e+00, 9.05800000e+03],\n",
      "       [1.71934557e+00, 9.05900000e+03],\n",
      "       [1.71934557e+00, 9.06000000e+03]]), array([[3.17862799e+00, 9.05700000e+03],\n",
      "       [3.17862632e+00, 9.05900000e+03],\n",
      "       [3.17862632e+00, 9.06000000e+03],\n",
      "       ...,\n",
      "       [3.17634371e+00, 1.20380000e+04],\n",
      "       [3.17634371e+00, 1.20390000e+04],\n",
      "       [3.17634371e+00, 1.20400000e+04]]), array([[3.15671477e+00, 1.11990000e+04],\n",
      "       [3.15671477e+00, 1.12000000e+04],\n",
      "       [3.15671477e+00, 1.12010000e+04],\n",
      "       ...,\n",
      "       [3.15415786e+00, 1.49800000e+04],\n",
      "       [3.15415499e+00, 1.49820000e+04],\n",
      "       [3.15415499e+00, 1.49830000e+04]]), array([[2.14505583e+00, 1.34940000e+04],\n",
      "       [2.14505571e+00, 1.34960000e+04],\n",
      "       [2.14505571e+00, 1.34970000e+04],\n",
      "       ...,\n",
      "       [2.14236327e+00, 1.79740000e+04],\n",
      "       [2.14236250e+00, 1.79760000e+04],\n",
      "       [2.14236173e+00, 1.79780000e+04]]), array([[1.74719193e+00, 1.57120000e+04],\n",
      "       [1.74719013e+00, 1.57140000e+04],\n",
      "       [1.74718979e+00, 1.57160000e+04],\n",
      "       ...,\n",
      "       [1.74523146e+00, 2.10140000e+04],\n",
      "       [1.74523146e+00, 2.10150000e+04],\n",
      "       [1.74523146e+00, 2.10160000e+04]]), array([[1.91499701e+00, 1.80300000e+04],\n",
      "       [1.91499402e+00, 1.80320000e+04],\n",
      "       [1.91499280e+00, 1.80340000e+04],\n",
      "       ...,\n",
      "       [1.91258451e+00, 2.39750000e+04],\n",
      "       [1.91258451e+00, 2.39760000e+04],\n",
      "       [1.91258451e+00, 2.39770000e+04]]), array([[2.50469518e+00, 2.03250000e+04],\n",
      "       [2.50469518e+00, 2.03260000e+04],\n",
      "       [2.50469472e+00, 2.03280000e+04],\n",
      "       ...,\n",
      "       [2.50097954e+00, 2.71200000e+04],\n",
      "       [2.50097954e+00, 2.71210000e+04],\n",
      "       [2.50097954e+00, 2.71220000e+04]]), array([[3.42597663e+00, 2.24570000e+04],\n",
      "       [3.42597663e+00, 2.24580000e+04],\n",
      "       [3.42597495e+00, 2.24600000e+04],\n",
      "       ...,\n",
      "       [3.41991516e+00, 2.99600000e+04],\n",
      "       [3.41991498e+00, 2.99620000e+04],\n",
      "       [3.41991491e+00, 2.99640000e+04]])]\n"
     ]
    }
   ],
   "source": [
    "print(ftcurve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708d5fcb",
   "metadata": {},
   "source": [
    "# Optimal Parameters for SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c56bf80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4991325998595476 0.5218054683588196\n"
     ]
    }
   ],
   "source": [
    "schedule = [mlrose_hiive.GeomDecay(init_temp = 10000, decay=0.9, min_temp=0.1),\n",
    "            mlrose_hiive.GeomDecay(init_temp = 1000000, decay=0.8, min_temp=0.1),\n",
    "            mlrose_hiive.GeomDecay(init_temp = 100000000, decay=0.7, min_temp=0.1),\n",
    "            mlrose_hiive.GeomDecay(init_temp = 10000000000, decay=0.6, min_temp=0.1),\n",
    "            mlrose_hiive.ArithDecay(init_temp=1000000, decay=0.9, min_temp=0.1)\n",
    "            mlrose_hiive.ArithDecay(init_temp=100000000, decay=0.8, min_temp=0.1)\n",
    "            mlrose_hiive.ArithDecay(init_temp=10000000000, decay=0.7, min_temp=0.1)\n",
    "            mlrose_hiive.ExpDecay(init_temp=1000000, exp_const=0.9, min_temp=0.1)\n",
    "            mlrose_hiive.ExpDecay(init_temp=100000000, exp_const=0.8, min_temp=0.1)\n",
    "            mlrose_hiive.ExpDecay(init_temp=10000000000, exp_const=0.7, min_temp=0.1)]\n",
    "            \n",
    "for i in range(0,10):\n",
    "    start_time = time.time()\n",
    "    nn_model_base = mlrose_hiive.NeuralNetwork([64,64,64,64], activation = 'tanh', \n",
    "                     algorithm = 'simulated_annealing', max_iters = 5000, \n",
    "                     bias = True, is_classifier = True, learning_rate = 0.0001, \n",
    "                     early_stopping = True, clip_max = 5, max_attempts = 100, \n",
    "                     random_state = 42, curve = True, restarts = 0, schedule = schedule[i])\n",
    "    val, train = strat_kfold_ann(nn_model_base, num_folds=5, x_train = x_train_w, y_train = y_train_w, dataset = 'wine', epochs = 5)\n",
    "    end_time = time.time()\n",
    "    wall_clock_time = end_time - start_time\n",
    "    print('schedule: ', schedule[i], 'validation recall: ', val , 'training recall: ', train,  'wall clock time: ', wall_clock_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20574c72",
   "metadata": {},
   "source": [
    "# Optimal Parameters for GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1a7f887d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 0. 0.]\n",
      "0       3\n",
      "1       3\n",
      "2       3\n",
      "3       3\n",
      "4       3\n",
      "       ..\n",
      "4893    3\n",
      "4894    2\n",
      "4895    3\n",
      "4896    4\n",
      "4897    3\n",
      "Name: quality, Length: 4898, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "pop_size = [100,300,500]\n",
    "mutation_prob = [0.1,0.3,0.5]\n",
    "\n",
    "for i in range(0,3):\n",
    "    for x in range(0,3):\n",
    "        start_time = time.time()\n",
    "        nn_model_base = mlrose_hiive.NeuralNetwork([64,64,64,64], activation = 'tanh', \n",
    "                         algorithm = 'genetic_alg', max_iters = 5000, \n",
    "                         bias = True, is_classifier = True, learning_rate = 0.0001, \n",
    "                         early_stopping = True, clip_max = 5, max_attempts = 100, \n",
    "                         random_state = 42, curve = True, pop_size = pop_size[i], mutation_prob = mutation_prob[x])\n",
    "        val, train = strat_kfold_ann(nn_model_base, num_folds=5, x_train = x_train_w, y_train = y_train_w, dataset = 'wine', epochs = 5)\n",
    "        end_time = time.time()\n",
    "        wall_clock_time = end_time - start_time\n",
    "        print('schedule: ', schedule[i], 'validation recall: ', val , 'training recall: ', train,  'wall clock time: ', wall_clock_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbf4deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [[64,64],[64,64,64],[128,128,128],[128,128,128,128]]\n",
    "activation = ['sigmoid','relu','tanh']\n",
    "learning_rate = [0.01,0.001,0.0001]\n",
    "\n",
    "for i in range(0,len(nodes)):\n",
    "    for x in range(0,len(activation)):\n",
    "        for y in range(0,len(learning_rate)):\n",
    "            nn_model_base = mlrose_hiive.NeuralNetwork(hidden_nodes = nodes[i], activation = activation[x], \n",
    "                                             algorithm = 'gradient_descent', max_iters = 1000, \n",
    "                                             bias = True, is_classifier = True, learning_rate = learning_rate[y], \n",
    "                                             early_stopping = True, clip_max = 5, max_attempts = 100, \n",
    "                                             random_state = 42, curve = True, restarts = 0)\n",
    "            \n",
    "            # One hot encoding\n",
    "            encoder = OneHotEncoder(sparse=False)\n",
    "            y_train_hot = encoder.fit_transform(y_train_sub.values.reshape(-1, 1))\n",
    "            y_val_hot = encoder.transform(y_val_sub.values.reshape(-1, 1))\n",
    "\n",
    "            predictions_train = clf.predict(x_train_w)\n",
    "            y_train_pred = np.argmax(predictions_train, axis=1)\n",
    "            \n",
    "            # Converting y_train_sub into the quality category \n",
    "            y_train_sub = np.argmax(y_train_hot, axis=1)\n",
    "            \n",
    "            # Make predictions on the validation set\n",
    "            predictions_val = clf.predict(X_val_sub)\n",
    "            y_val_pred = np.argmax(predictions_val, axis=1)\n",
    "            \n",
    "            # Converting y_train_sub into the quality category \n",
    "            y_val_sub = np.argmax(y_val_hot, axis=1)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
